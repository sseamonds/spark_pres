{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Spark 2.x one can work primarily through the SparkSession\n",
    "### which avoids need for managing SparkContext, SQLContext and HiveContext\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.SparkSession<br><br>\n",
    "val sparkSession = SparkSession.builder().appName(\"Spark SQL basic example\").getOrCreate()<br><br>\n",
    "sparkSession.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",<br> \"true\").load(\"raw_session_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// SparkContext typically already available in notebooks, we'll use it for this demo\n",
    "\n",
    "sc.getClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical ML problem...\n",
    "### Binary Classification\n",
    "#### - bot or not\n",
    "#### - churn\n",
    "#### - fraud\n",
    "#### - medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw session data into a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+----------+\n",
      "|session_id|user_id|     action|product_id|\n",
      "+----------+-------+-----------+----------+\n",
      "|         1|      1|       view|    prod_1|\n",
      "|         1|      1|   cart_add|    prod_1|\n",
      "|         1|      1|cart_remove|    prod_1|\n",
      "|         1|      1|       view|    prod_2|\n",
      "|         1|      1|       view|    prod_3|\n",
      "|         1|      1|       view|    prod_4|\n",
      "|         1|      1|       view|    prod_2|\n",
      "|         2|      1|       view|    prod_5|\n",
      "|         2|      1|     search|      null|\n",
      "|         2|      1|       view|    prod_6|\n",
      "|         2|      1|     search|      null|\n",
      "|         2|      1|       view|    prod_2|\n",
      "|         3|      1|       view|    prod_5|\n",
      "|         3|      1|     search|      null|\n",
      "|         3|      1|   cart_add|    prod_2|\n",
      "|         3|      1|       view|    prod_8|\n",
      "|         3|      1|   purchase|    prod_2|\n",
      "+----------+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQLContext is a wrapper on SparkContext which allows sql-like operations\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// read raw session events from .csv as a Dataset\n",
    "val sessionsDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"raw_session_data_sample.csv\")\n",
    "\n",
    "sessionsDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the raw data should look like after featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+------------+--------------+-----------------+-----------+\n",
      "|prod_view_count|unique_prod_view_count|search_count|cart_add_count|cart_remove_count|is_purchase|\n",
      "+---------------+----------------------+------------+--------------+-----------------+-----------+\n",
      "|              5|                     4|           0|             1|                1|          0|\n",
      "|              3|                     3|           2|             0|                0|          0|\n",
      "|              2|                     2|           1|             1|                0|          1|\n",
      "+---------------+----------------------+------------+--------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sessionsFeaturizedDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"raw_session_data_sample_featurized.csv\")\n",
    "\n",
    "sessionsFeaturizedDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(view,prod_1))\n",
      "(1,(cart_add,prod_1))\n",
      "(1,(cart_remove,prod_1))\n",
      "(1,(view,prod_2))\n",
      "(1,(view,prod_3))\n",
      "(1,(view,prod_4))\n",
      "(1,(view,prod_2))\n",
      "(2,(view,prod_5))\n",
      "(2,(search,NA))\n",
      "(2,(view,prod_6))\n",
      "(2,(search,NA))\n",
      "(2,(view,prod_2))\n",
      "(3,(view,prod_5))\n",
      "(3,(search,NA))\n",
      "(3,(cart_add,prod_2))\n",
      "(3,(view,prod_8))\n",
      "(3,(purchase,prod_2))\n"
     ]
    }
   ],
   "source": [
    "// RDD's are NOT deprecated\n",
    "// RDD's are immutable\n",
    "// lazily evaluated\n",
    "// Lend themselves to unstructured/schemaless data\n",
    "\n",
    "// Get count of product views per session - RDD version \n",
    "val NA_STRING  = \"NA\"\n",
    "\n",
    "// Easy to switch from DS to RDD (and back)\n",
    "// RDD of Row objects\n",
    "val sessionsRDD = sessionsDS.rdd\n",
    "\n",
    "// Rekey the session data in format : (session_id, (action, prod_id))\n",
    "val sessionsRDDKeyed = sessionsRDD.map(r => (r.getInt(r.fieldIndex(\"session_id\")), (r.getString(r.fieldIndex(\"action\")), Option(r.get(r.fieldIndex(\"product_id\"))).getOrElse(NA_STRING))))\n",
    "\n",
    "// foreach() allows to execute non-mutating operations on each item, such as println()\n",
    "sessionsRDDKeyed.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap (action, product) tuples in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,List((view,prod_1)))\n",
      "(1,List((cart_add,prod_1)))\n",
      "(1,List((cart_remove,prod_1)))\n",
      "(1,List((view,prod_2)))\n",
      "(1,List((view,prod_3)))\n",
      "(1,List((view,prod_4)))\n",
      "(1,List((view,prod_2)))\n",
      "(2,List((view,prod_5)))\n",
      "(2,List((search,NA)))\n",
      "(2,List((view,prod_6)))\n",
      "(2,List((search,NA)))\n",
      "(2,List((view,prod_2)))\n",
      "(3,List((view,prod_5)))\n",
      "(3,List((search,NA)))\n",
      "(3,List((cart_add,prod_2)))\n",
      "(3,List((view,prod_8)))\n",
      "(3,List((purchase,prod_2)))\n"
     ]
    }
   ],
   "source": [
    "val sessionsRDDKeyedWithList = sessionsRDDKeyed.map(x => (x._1, List(x._2)))\n",
    "\n",
    "sessionsRDDKeyedWithList.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate List(Tuple)) objects into a single List using reduceByKey\n",
    "### reduceByKey is commutative (a + b = b + a) and associative ((a + b) + c = d = a + (b + c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,List((view,prod_1), (cart_add,prod_1), (cart_remove,prod_1), (view,prod_2), (view,prod_3), (view,prod_4), (view,prod_2)))\n",
      "(3,List((view,prod_5), (search,NA), (cart_add,prod_2), (view,prod_8), (purchase,prod_2)))\n",
      "(2,List((view,prod_5), (search,NA), (view,prod_6), (search,NA), (view,prod_2)))\n"
     ]
    }
   ],
   "source": [
    "val sessionsRDDReduced = sessionsRDDKeyedWithList.reduceByKey(_ ++ _)\n",
    "\n",
    "sessionsRDDReduced.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 'view' action items only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,List((view,prod_1), (view,prod_2), (view,prod_3), (view,prod_4), (view,prod_2)))\n",
      "(3,List((view,prod_5), (view,prod_8)))\n",
      "(2,List((view,prod_5), (view,prod_6), (view,prod_2)))\n"
     ]
    }
   ],
   "source": [
    "val sessionsRDDProductViews = sessionsRDDReduced.map(x => (x._1, x._2.filter(y => y._1 == \"view\")))\n",
    "\n",
    "sessionsRDDProductViews.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of product view by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,5)\n",
      "(3,2)\n",
      "(2,3)\n"
     ]
    }
   ],
   "source": [
    "val sessionIdWithProductViewCount = sessionsRDDProductViews.map(x => (x._1, x._2.size))\n",
    "\n",
    "sessionIdWithProductViewCount.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of UNIQUE product view by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,4)\n",
      "(3,2)\n",
      "(2,3)\n"
     ]
    }
   ],
   "source": [
    "val sessionIdWithUniqueProductViewCount = sessionsRDDProductViews.map(x => (x._1, x._2.map(y => y._2).distinct.size))\n",
    "\n",
    "sessionIdWithUniqueProductViewCount.collect.foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization using Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Dataframes are new with 1.3\n",
    "// Inspired by dataframe construct in R and python\n",
    "// Lacking in type safety (refer to columns by string name, without type-based getter)\n",
    "\n",
    "// Datasets are new with 1.6, stable with 2.0\n",
    "// still an RDD 'under the hood'\n",
    "// compile-time type-safety, more errors caught at compile time than with DataFrame\n",
    "// More sql-like syntax, even alows actual sql statements\n",
    "// Allows to refer to columns/fields by name\n",
    "\n",
    "sessionsDS.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get product view count using DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \\$\"field_name\" syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|session_id|prod_view_count|\n",
      "+----------+---------------+\n",
      "|         1|              5|\n",
      "|         3|              2|\n",
      "|         2|              3|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlContext.implicits._\n",
    "\n",
    "var sessionWithProductViewCountDS = sessionsDS.filter($\"action\" === \"view\").groupBy(\"session_id\").count()\n",
    "sessionWithProductViewCountDS = sessionWithProductViewCountDS.withColumnRenamed(\"count\", \"prod_view_count\")\n",
    "\n",
    "sessionWithProductViewCountDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Row.get() (more type-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|session_id|prod_view_count|\n",
      "+----------+---------------+\n",
      "|         1|              5|\n",
      "|         3|              2|\n",
      "|         2|              3|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var sessionWithProductViewCountDS = sessionsDS.filter(r => r.getString(r.fieldIndex(\"action\")) == \"view\").groupBy(\"session_id\").count()\n",
    "sessionWithProductViewCountDS = sessionWithProductViewCountDS.withColumnRenamed(\"count\", \"prod_view_count\")\n",
    "\n",
    "sessionWithProductViewCountDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|session_id|prod_view_count|\n",
      "+----------+---------------+\n",
      "|         1|              5|\n",
      "|         3|              2|\n",
      "|         2|              3|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Register the DataFrame as a SQL temporary view\n",
    "sessionsDS.createOrReplaceTempView(\"sessions_table\")\n",
    "\n",
    "sqlContext.sql(\"SELECT session_id, count(*) as prod_view_count FROM sessions_table where action = 'view' group by session_id\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of searches per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|search_session_id|search_count|\n",
      "+-----------------+------------+\n",
      "|                3|           1|\n",
      "|                2|           2|\n",
      "+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var sessionWithSearchCountDS = sessionsDS.filter($\"action\" === \"search\").groupBy(\"session_id\").count()\n",
    "\n",
    "sessionWithSearchCountDS = sessionWithSearchCountDS.withColumnRenamed(\"session_id\", \"search_session_id\")\n",
    "sessionWithSearchCountDS = sessionWithSearchCountDS.withColumnRenamed(\"count\", \"search_count\")\n",
    "\n",
    "sessionWithSearchCountDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can join Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+\n",
      "|session_id|prod_view_count|search_count|\n",
      "+----------+---------------+------------+\n",
      "|         1|              5|           0|\n",
      "|         3|              2|           1|\n",
      "|         2|              3|           2|\n",
      "+----------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var sessionsWithBothCountsDS = sessionWithProductViewCountDS.join(sessionWithSearchCountDS, sessionWithProductViewCountDS(\"session_id\") === sessionWithSearchCountDS(\"search_session_id\"), \"left_outer\")\n",
    "\n",
    "// Did a left join so fill in defaults for missing values\n",
    "// since Datasets are immutable must reassign to a new DS\n",
    "val finalDS = sessionsWithBothCountsDS.na.fill(0)\n",
    "\n",
    "// drop duplicate col from join\n",
    "finalDS.drop(\"search_session_id\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions can be applied to columns to derive new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+----------+-----------+\n",
      "|session_id|user_id|     action|product_id|is_purchase|\n",
      "+----------+-------+-----------+----------+-----------+\n",
      "|         1|      1|       view|    prod_1|          0|\n",
      "|         1|      1|   cart_add|    prod_1|          0|\n",
      "|         1|      1|cart_remove|    prod_1|          0|\n",
      "|         1|      1|       view|    prod_2|          0|\n",
      "|         1|      1|       view|    prod_3|          0|\n",
      "|         1|      1|       view|    prod_4|          0|\n",
      "|         1|      1|       view|    prod_2|          0|\n",
      "|         2|      1|       view|    prod_5|          0|\n",
      "|         2|      1|     search|      null|          0|\n",
      "|         2|      1|       view|    prod_6|          0|\n",
      "|         2|      1|     search|      null|          0|\n",
      "|         2|      1|       view|    prod_2|          0|\n",
      "|         3|      1|       view|    prod_5|          0|\n",
      "|         3|      1|     search|      null|          0|\n",
      "|         3|      1|   cart_add|    prod_2|          0|\n",
      "|         3|      1|       view|    prod_8|          0|\n",
      "|         3|      1|   purchase|    prod_2|          1|\n",
      "+----------+-------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def isPurchase(action: String) : Int = if (action.equals(\"purchase\")) 1 else 0\n",
    "\n",
    "// functional mapping from scala function to a UDF\n",
    "val isPurchaseUDF = udf((s: String) => isPurchase(s))\n",
    "\n",
    "var sessionWithIsPurchaseDS = sessionsDS.withColumn(\"is_purchase\", isPurchaseUDF($\"action\"))\n",
    "\n",
    "sessionWithIsPurchaseDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then use group-by/max to assign the label (is_purchase) to the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|session_id|max(is_purchase)|\n",
      "+----------+----------------+\n",
      "|         1|               0|\n",
      "|         3|               1|\n",
      "|         2|               0|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "\n",
    "sessionWithIsPurchaseDS.groupBy($\"session_id\").agg(max(\"is_purchase\").alias(\"is_purchase\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
