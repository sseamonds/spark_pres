{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## org.apache.spark.mllib package offers :\n",
    "#### Encoding/Transformation\n",
    "#### Tuning/Evaluation\n",
    "#### Hashing and Feature Reduction\n",
    "#### Classification (trees, linear, probabilistic, ensemble)\n",
    "#### Clustering\n",
    "#### Regression\n",
    "##### see : https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "#### - bot or not\n",
    "#### - churn\n",
    "#### - fraud\n",
    "#### - medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers\n",
    "### See : https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "   <td><img src=\"linear_separator_1.png\",width=300,height=300></td>\n",
    "   <td><img src=\"linear_separator_2.png\",width=300,height=300></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uses linear combination of weighted features to establish a 'decision boundary' :\n",
    "#### y = wx + b\n",
    "#### y : the label\n",
    "#### b : intercept, w_0 \n",
    "#### w : coefficients, (w_1, w_2, ....w_n) where n = number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "|cart_add_count|cart_remove_count|prod_views|prod_views_unique|search_count|is_purchase|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "|             4|                7|         0|                0|           3|          0|\n",
      "|             5|                6|        12|               11|           9|          1|\n",
      "|             9|                2|         8|                8|           8|          1|\n",
      "|             6|                7|         4|                3|           0|          0|\n",
      "|             2|                9|         5|                5|           1|          0|\n",
      "|             1|                5|         0|                0|           0|          0|\n",
      "|             9|                6|        14|               14|           7|          1|\n",
      "|             9|                3|         2|                2|           1|          0|\n",
      "|             9|                5|         1|                1|           2|          0|\n",
      "|             8|                9|         4|                4|           0|          0|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val trainingDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").//\n",
    "option(\"inferSchema\", \"true\").load(\"session_training_data.csv\")\n",
    "\n",
    "trainingDS.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "|cart_add_count|cart_remove_count|prod_views|prod_views_unique|search_count|is_purchase|            features|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "|             4|                7|         0|                0|           3|          0|[4.0,7.0,0.0,0.0,...|\n",
      "|             5|                6|        12|               11|           9|          1|[5.0,6.0,12.0,11....|\n",
      "|             9|                2|         8|                8|           8|          1|[9.0,2.0,8.0,8.0,...|\n",
      "|             6|                7|         4|                3|           0|          0|[6.0,7.0,4.0,3.0,...|\n",
      "|             2|                9|         5|                5|           1|          0|[2.0,9.0,5.0,5.0,...|\n",
      "|             1|                5|         0|                0|           0|          0| (5,[0,1],[1.0,5.0])|\n",
      "|             9|                6|        14|               14|           7|          1|[9.0,6.0,14.0,14....|\n",
      "|             9|                3|         2|                2|           1|          0|[9.0,3.0,2.0,2.0,...|\n",
      "|             9|                5|         1|                1|           2|          0|[9.0,5.0,1.0,1.0,...|\n",
      "|             8|                9|         4|                4|           0|          0|[8.0,9.0,4.0,4.0,...|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val LABEL_COL = \"is_purchase\"\n",
    "val FEATURE_COLS = Array(\"cart_add_count\", \"cart_remove_count\", \"prod_views\", \"prod_views_unique\", \"search_count\")\n",
    "\n",
    "// VectorAssembler collects features of a Dataset into a sparse vector, stored in \n",
    "// a single column, which most ML algos require\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(FEATURE_COLS).setOutputCol(\"features\")\n",
    "\n",
    "val trainingDSTransformed = assembler.transform(trainingDS)\n",
    "\n",
    "trainingDSTransformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression model from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07068837177379167,-0.12516099365830843,0.11706925017163039,0.11853810891596289,0.18701273424465098] Intercept: -3.114507068635333\n"
     ]
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setLabelCol(LABEL_COL).setFeaturesCol(\"features\").setRegParam(0.2)\n",
    "\n",
    "// Fit the model\n",
    "val lrModel = lr.fit(trainingDSTransformed)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model on test data and see how accurate our predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9\n"
     ]
    }
   ],
   "source": [
    "val testDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").//\n",
    "option(\"inferSchema\", \"true\").load(\"session_test_data.csv\")\n",
    "val testDSTransformed = assembler.transform(testDS)\n",
    "\n",
    "val predictionsDS = lrModel.transform(testDSTransformed)\n",
    "\n",
    "// BinaryClassificationEvaluator only gives AUC/AUPR, \n",
    "// using MulticlassClassificationEvaluator for accuracy\n",
    "val evaluator = new MulticlassClassificationEvaluator().//\n",
    "setLabelCol(\"is_purchase\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "\n",
    "val acc = evaluator.evaluate(predictionsDS)\n",
    "println(s\"accuracy : ${acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|is_purchase|prediction|\n",
      "+-----------+----------+\n",
      "|          0|       0.0|\n",
      "|          0|       0.0|\n",
      "|          0|       0.0|\n",
      "|          1|       0.0|\n",
      "|          0|       0.0|\n",
      "|          1|       1.0|\n",
      "|          0|       0.0|\n",
      "|          0|       0.0|\n",
      "|          1|       0.0|\n",
      "|          1|       1.0|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDS.select(\"is_purchase\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Using Spark Pipeline\n",
    "### See : https://spark.apache.org/docs/2.2.0/ml-pipeline.html\n",
    "\n",
    "### Here we use the Pipeline construct to string multiple transformations together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(FEATURE_COLS).setOutputCol(\"features\")\n",
    "\n",
    "// scales data by Standard Deviation\n",
    "val scaler = new org.apache.spark.ml.feature.StandardScaler().setInputCol(\"features\").//\n",
    "setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n",
    "\n",
    "val logisticRegression = new LogisticRegression().setLabelCol(\"is_purchase\").//\n",
    "setFeaturesCol(\"scaledFeatures\").setRegParam(0.2)\n",
    "\n",
    "// Output of each pipeline stage is input to next\n",
    "// No need to explicitly call transform for each component\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, scaler, logisticRegression))\n",
    "\n",
    "val lrModel = pipeline.fit(trainingDS)\n",
    "\n",
    "// run model on unseen test data\n",
    "val pipeLinepredictions = lrModel.transform(testDS)\n",
    "\n",
    "val acc = evaluator.evaluate(pipeLinepredictions)\n",
    "println(s\"accuracy : ${acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "### See : https://spark.apache.org/docs/2.2.0/mllib-decision-tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "   <td><img src=\"decision_tree_1.jpg\",width=420,height=350></td>\n",
    "   <td><img src=\"decision_tree_2.png\",width=400,height=300></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val intToDble = udf[Double, Integer]( _.toDouble)\n",
    "val trainingDSTransformedForDT = trainingDSTransformed.withColumn(\"is_purchase_double\",//\n",
    "                                                                  intToDble(trainingDSTransformed(LABEL_COL)))\n",
    "\n",
    "// DT Throws err when label is not Double\n",
    "val dt = new DecisionTreeClassifier().setLabelCol(\"is_purchase_double\").setFeaturesCol(\"features\")\n",
    "\n",
    "// Fit the model\n",
    "val dtModel = dt.fit(trainingDSTransformedForDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply model to test data\n",
    "val testDSTransformedForDT = testDSTransformed.withColumn(\"is_purchase_double\",//\n",
    "                                                                  intToDble(testDSTransformed(LABEL_COL)))\n",
    "val dtPredictions = dtModel.transform(testDSTransformedForDT)\n",
    "\n",
    "val acc = evaluator.evaluate(dtPredictions)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation with a Random Forest classifier\n",
    "### See : https://spark.apache.org/docs/2.2.0/ml-tuning.html\n",
    "### See  : https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#random-forest-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"is_purchase_double\").setFeaturesCol(\"features\")\n",
    "\n",
    "// ParamGrid controls the param combos, param combos can multiply REAL fast\n",
    "// Adding one more param takes the # of combos from 48 to 64\n",
    "val rfParamGrid = new ParamGridBuilder().//\n",
    "addGrid(rf.maxDepth, Array(2, 3, 4)).//\n",
    "addGrid(rf.maxBins, Array(2, 4, 8, 12)).//\n",
    "addGrid(rf.numTrees, Array(2, 4, 8, 12)).build()\n",
    "\n",
    "// CrossValidator itself is an Estimator...\n",
    "val rfCV = new CrossValidator().setEstimator(rf).//\n",
    "setEvaluator(new MulticlassClassificationEvaluator().setLabelCol(\"is_purchase_double\").//\n",
    "             setPredictionCol(\"prediction\").setMetricName(\"accuracy\")).//\n",
    "setEstimatorParamMaps(rfParamGrid).setNumFolds(3)\n",
    "\n",
    "// ... and thus has a fit method. fits over each param combo, over K folds each\n",
    "val rfCVModel = rfCV.fit(trainingDSTransformedForDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what params produced the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\trfc_fbaadfa046ba-cacheNodeIds: false,\n",
      "\trfc_fbaadfa046ba-checkpointInterval: 10,\n",
      "\trfc_fbaadfa046ba-featureSubsetStrategy: auto,\n",
      "\trfc_fbaadfa046ba-featuresCol: features,\n",
      "\trfc_fbaadfa046ba-impurity: gini,\n",
      "\trfc_fbaadfa046ba-labelCol: is_purchase_double,\n",
      "\trfc_fbaadfa046ba-maxBins: 8,\n",
      "\trfc_fbaadfa046ba-maxDepth: 3,\n",
      "\trfc_fbaadfa046ba-maxMemoryInMB: 256,\n",
      "\trfc_fbaadfa046ba-minInfoGain: 0.0,\n",
      "\trfc_fbaadfa046ba-minInstancesPerNode: 1,\n",
      "\trfc_fbaadfa046ba-predictionCol: prediction,\n",
      "\trfc_fbaadfa046ba-probabilityCol: probability,\n",
      "\trfc_fbaadfa046ba-rawPredictionCol: rawPrediction,\n",
      "\trfc_fbaadfa046ba-seed: 207336481,\n",
      "\trfc_fbaadfa046ba-subsamplingRate: 1.0\n",
      "}\n",
      "numTrees : 4\n"
     ]
    }
   ],
   "source": [
    "println(rfCVModel.bestModel.extractParamMap())\n",
    "println(\"numTrees : \" + rfCVModel.bestModel.asInstanceOf[RandomForestClassificationModel].getNumTrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### See how the best model does on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfPredictions = rfCVModel.bestModel.transform(testDSTransformed)\n",
    "\n",
    "val acc = evaluator.evaluate(rfPredictions)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
