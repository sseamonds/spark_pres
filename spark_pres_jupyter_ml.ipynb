{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// todo : maybe add club O and use onehot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers\n",
    "### See : https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "   <td><img src=\"linear_separator_1.png\",width=300,height=300></td>\n",
    "   <td><img src=\"linear_separator_2.png\",width=300,height=300></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "|cart_add_count|cart_remove_count|prod_views|prod_views_unique|search_count|is_purchase|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "|             4|                7|         0|                0|           3|          0|\n",
      "|             5|                6|        12|               11|           9|          1|\n",
      "|             9|                2|         8|                8|           8|          1|\n",
      "|             6|                7|         4|                3|           0|          0|\n",
      "|             2|                9|         5|                5|           1|          0|\n",
      "|             1|                5|         0|                0|           0|          0|\n",
      "|             9|                6|        14|               14|           7|          1|\n",
      "|             9|                3|         2|                2|           1|          0|\n",
      "|             9|                5|         1|                1|           2|          0|\n",
      "|             8|                9|         4|                4|           0|          0|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val trainingDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"session_training_data.csv\")\n",
    "\n",
    "trainingDS.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "|cart_add_count|cart_remove_count|prod_views|prod_views_unique|search_count|is_purchase|            features|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "|             4|                7|         0|                0|           3|          0|[4.0,7.0,0.0,0.0,...|\n",
      "|             5|                6|        12|               11|           9|          1|[5.0,6.0,12.0,11....|\n",
      "|             9|                2|         8|                8|           8|          1|[9.0,2.0,8.0,8.0,...|\n",
      "|             6|                7|         4|                3|           0|          0|[6.0,7.0,4.0,3.0,...|\n",
      "|             2|                9|         5|                5|           1|          0|[2.0,9.0,5.0,5.0,...|\n",
      "|             1|                5|         0|                0|           0|          0| (5,[0,1],[1.0,5.0])|\n",
      "|             9|                6|        14|               14|           7|          1|[9.0,6.0,14.0,14....|\n",
      "|             9|                3|         2|                2|           1|          0|[9.0,3.0,2.0,2.0,...|\n",
      "|             9|                5|         1|                1|           2|          0|[9.0,5.0,1.0,1.0,...|\n",
      "|             8|                9|         4|                4|           0|          0|[8.0,9.0,4.0,4.0,...|\n",
      "+--------------+-----------------+----------+-----------------+------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val LABEL_COL = \"is_purchase\"\n",
    "val FEATURE_COLS = Array(\"cart_add_count\", \"cart_remove_count\", \"prod_views\", \"prod_views_unique\", \"search_count\")\n",
    "\n",
    "// VectorAssembler collects features of a Dataset into a sparse vector, stored in \n",
    "// a single column, which most ML algos require\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(FEATURE_COLS).setOutputCol(\"features\")\n",
    "\n",
    "val trainingDSTransformed = assembler.transform(trainingDS)\n",
    "\n",
    "trainingDSTransformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression model from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07068837177379167,-0.12516099365830843,0.11706925017163039,0.11853810891596289,0.18701273424465098] Intercept: -3.114507068635333\n"
     ]
    }
   ],
   "source": [
    "val lr = new LogisticRegression().setLabelCol(\"is_purchase\").setRegParam(0.2).setFeaturesCol(\"features\")\n",
    "\n",
    "// Fit the model\n",
    "val lrModel = lr.fit(trainingDSTransformed)\n",
    "\n",
    "// Print the coefficients and intercept for logistic regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model on test data and see how accurate our predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9\n"
     ]
    }
   ],
   "source": [
    "val testDS = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"session_test_data.csv\")\n",
    "val testDSTransformed = assembler.transform(testDS)\n",
    "\n",
    "val predictions = lrModel.transform(testDSTransformed)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"is_purchase\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "\n",
    "val acc = evaluator.evaluate(predictions)\n",
    "//predictions.select(\"is_purchase\", \"prediction\").show(30)\n",
    "println(s\"accuracy : ${acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Using Spark Pipeline\n",
    "### See : https://spark.apache.org/docs/2.2.0/ml-pipeline.html\n",
    "\n",
    "### Here we use the Pipeline construct to string transformations together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(FEATURE_COLS).setOutputCol(\"features\")\n",
    "\n",
    "// scales data by Standard Deviation\n",
    "val scaler = new org.apache.spark.ml.feature.StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\").setWithStd(true).setWithMean(false)\n",
    "\n",
    "val logisticRegression = new LogisticRegression().setLabelCol(\"is_purchase\").setRegParam(0.2).setFeaturesCol(\"scaledFeatures\")\n",
    "\n",
    "// Output of each pipeline stage is input to next\n",
    "// No need to explicitly call transform for each component\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, scaler, logisticRegression))\n",
    "\n",
    "val logisticRegressionModel = pipeline.fit(trainingDS)\n",
    "\n",
    "// run model on unseen test data\n",
    "val pipeLinepredictions = logisticRegressionModel.transform(testDS)\n",
    "val acc = evaluator.evaluate(pipeLinepredictions)\n",
    "println(s\"accuracy : ${acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "### See : https://spark.apache.org/docs/2.2.0/mllib-decision-tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "   <td><img src=\"decision_tree_1.jpg\",width=420,height=350></td>\n",
    "   <td><img src=\"decision_tree_2.png\",width=400,height=300></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val intToDble = udf[Double, Integer]( _.toDouble)\n",
    "val trainingDSTransformedForDT = trainingDSTransformed.withColumn(\"is_purchase_double\", intToDble(trainingDSTransformed(\"is_purchase\")))\n",
    "\n",
    "// DT Throws err when label is not Double\n",
    "val dt = new DecisionTreeClassifier().setLabelCol(\"is_purchase_double\").setFeaturesCol(\"features\")\n",
    "\n",
    "// Fit the model\n",
    "val dtModel = dt.fit(trainingDSTransformedForDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply mode to test data\n",
    "\n",
    "val dtPredictions = dtModel.transform(testDSTransformed)\n",
    "\n",
    "val acc = evaluator.evaluate(dtPredictions)\n",
    "// dtPredictions.select(\"is_purchase\", \"prediction\").show(30)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "### see : https://spark.apache.org/docs/2.2.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"is_purchase_double\").setFeaturesCol(\"features\")\n",
    "\n",
    "// ParamGrid controls the param combos, param combos can multiply REAL fast\n",
    "val rfParamGrid = new ParamGridBuilder().addGrid(rf.maxDepth, Array(2, 3, 4)).addGrid(rf.maxBins, Array(2, 4, 8)).addGrid(rf.numTrees, Array(4, 8, 12)).build()\n",
    "\n",
    "// CrossValidator itself is an Estimator...\n",
    "val rfCV = new CrossValidator().setEstimator(rf).setEvaluator(new MulticlassClassificationEvaluator().setLabelCol(\"is_purchase_double\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")).setEstimatorParamMaps(rfParamGrid).setNumFolds(3)\n",
    "\n",
    "// ... and thus has a fit method, fits over each param combo, over K folds each\n",
    "val rfCVModel = rfCV.fit(trainingDSTransformedForDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the best model does on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfPredictions = rfCVModel.bestModel.transform(testDSTransformed)\n",
    "\n",
    "val acc = evaluator.evaluate(rfPredictions)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what params produced the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\trfc_a832eab1201b-cacheNodeIds: false,\n",
      "\trfc_a832eab1201b-checkpointInterval: 10,\n",
      "\trfc_a832eab1201b-featureSubsetStrategy: auto,\n",
      "\trfc_a832eab1201b-featuresCol: features,\n",
      "\trfc_a832eab1201b-impurity: gini,\n",
      "\trfc_a832eab1201b-labelCol: is_purchase_double,\n",
      "\trfc_a832eab1201b-maxBins: 8,\n",
      "\trfc_a832eab1201b-maxDepth: 3,\n",
      "\trfc_a832eab1201b-maxMemoryInMB: 256,\n",
      "\trfc_a832eab1201b-minInfoGain: 0.0,\n",
      "\trfc_a832eab1201b-minInstancesPerNode: 1,\n",
      "\trfc_a832eab1201b-predictionCol: prediction,\n",
      "\trfc_a832eab1201b-probabilityCol: probability,\n",
      "\trfc_a832eab1201b-rawPredictionCol: rawPrediction,\n",
      "\trfc_a832eab1201b-seed: 207336481,\n",
      "\trfc_a832eab1201b-subsamplingRate: 1.0\n",
      "}\n",
      "numTrees : 4\n"
     ]
    }
   ],
   "source": [
    "println(rfCVModel.bestModel.extractParamMap())\n",
    "println(\"numTrees : \" + rfCVModel.bestModel.asInstanceOf[RandomForestClassificationModel].getNumTrees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
